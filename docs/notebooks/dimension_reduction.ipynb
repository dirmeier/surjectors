{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f6f4229-6b15-4e2b-89af-8957708479d7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Dimensionality reduction with surjections\n",
    "\n",
    "Surjective normalizing flows use dimensionality-reducing transformations instead of dimensionality-preserving bijective ones. \n",
    "Below we implement several surjective normalizing for a density estimation problem and compare them to a conventional bijective flow.\n",
    "\n",
    "Interactive online version:\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/dirmeier/surjectors/blob/main/docs/notebooks/dimension_reduction.ipynb\">\n",
    "    <img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"vertical-align:text-bottom\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6562e4c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import distrax\n",
    "import haiku as hk\n",
    "import optax\n",
    "import jax\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import namedtuple\n",
    "from jax import jit\n",
    "from jax import numpy as jnp\n",
    "from jax import random as jr\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from surjectors import TransformedDistribution, Chain, MaskedCoupling\n",
    "from surjectors import MaskedCouplingInferenceFunnel, MLPInferenceFunnel, LULinear\n",
    "from surjectors.nn import MADE, make_mlp\n",
    "from surjectors.util import as_batch_iterator, unstack, make_alternating_binary_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2660da",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We define a training function first that we can use for all density estimation tasks below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abf63596-15e8-4777-bc04-9784068deb63",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(rng_key, data, model, n_iter=1000):\n",
    "    # convert the data set to an iterator\n",
    "    batch_key, rng_key = jr.split(rng_key)\n",
    "    train_iter = as_batch_iterator(batch_key, data, 100, True)\n",
    "\n",
    "    # initialize the model\n",
    "    init_key, rng_key = jr.split(rng_key)\n",
    "    params = model.init(init_key, method=\"log_prob\", **train_iter(0))\n",
    "\n",
    "    # create an optimizer\n",
    "    optimizer = optax.adam(1e-4)\n",
    "    state = optimizer.init(params)\n",
    "\n",
    "    @jit\n",
    "    # gradient step\n",
    "    def step(params, state, **batch):\n",
    "        def loss_fn(params):\n",
    "            lp = model.apply(params, None, method=\"log_prob\", **batch)\n",
    "            return -jnp.mean(lp)\n",
    "\n",
    "        loss, grads = jax.value_and_grad(loss_fn)(params)\n",
    "        updates, new_state = optimizer.update(grads, state, params)\n",
    "        new_params = optax.apply_updates(params, updates)\n",
    "        return loss, new_params, new_state\n",
    "\n",
    "    losses = np.zeros(n_iter)\n",
    "    # training loop\n",
    "    for i in tqdm(range(n_iter)):\n",
    "        train_loss = 0.0\n",
    "        # iterate over batches\n",
    "        for j in range(train_iter.num_batches):\n",
    "            batch = train_iter(j)\n",
    "            batch_loss, params, state = step(params, state, **batch)\n",
    "            train_loss += batch_loss\n",
    "        losses[i] = train_loss\n",
    "\n",
    "    return params, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ca6e01-9ee1-4761-9ac9-7000d770bfdd",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## Data\n",
    "\n",
    "We simulate data from a factor model for testing. The data can should easily embedded in a lower dimensional space via a linear transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f01a1fb6-a780-4a2b-b4d6-2e0bf48e15be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rng_key_seq = hk.PRNGSequence(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97896881-6641-48de-b170-2361fa1ab9d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_train, n_test = 1000, 200\n",
    "n = n_train + n_test\n",
    "p_data, p_latent = 20, 5\n",
    "\n",
    "z = jr.normal(next(rng_key_seq), (n, p_latent))\n",
    "W = jr.normal(next(rng_key_seq), (p_data, p_latent)) * 0.1\n",
    "y = (W @ z.T).T + jr.normal(next(rng_key_seq), (n, p_data)) * 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4433d9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## A bijective baseline\n",
    "\n",
    "We start with a simple baseline: a masked coupling flow with rational quadratic splines as transforms. The data is not dimensionality-reducing and hence tries to estimate the density on the $20$-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435ad2b6-2e33-45e0-bd10-66f8b574e413",
   "metadata": {
    "tags": []
   },
   "source": [
    "A RQ splince flow requires defining ranges for which we use the lower and upper bounds of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9cd8194",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "range_min, range_max = float(np.min(y)), float(np.max(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168a26ee-da57-4c7c-9dcb-34f4aff6f366",
   "metadata": {},
   "source": [
    "Next we define the conditioner function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53ad72b4-b4f7-4eca-b169-af17509c2887",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_rq_conditioner(event_shape, hidden_sizes, n_bins):\n",
    "    n_params = 3 * n_bins + 1\n",
    "    return hk.Sequential([\n",
    "        make_mlp(hidden_sizes + [event_shape * n_params]),\n",
    "        hk.Reshape((event_shape,) + (n_params,), preserve_dims=-1),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4747321-be44-4ad4-bd3a-cbf80915442f",
   "metadata": {},
   "source": [
    "We create a baseine that uses five masked coupling flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "394d0646-e1b6-4794-9164-5567d2118f8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_baseline(n_dimensions):\n",
    "    def flow(**kwargs):\n",
    "        def bijector_fn(params):\n",
    "            return distrax.RationalQuadraticSpline(params, range_min=range_min, range_max=range_max)\n",
    "\n",
    "        layers = []\n",
    "        for i in range(5):\n",
    "            layer = MaskedCoupling(\n",
    "                mask=make_alternating_binary_mask(n_dimensions, i % 2 == 0),\n",
    "                conditioner=make_rq_conditioner(n_dimensions, [128, 128], 4),\n",
    "                bijector_fn=bijector_fn,\n",
    "            )\n",
    "            layers.append(layer)\n",
    "\n",
    "        transform = Chain(layers)\n",
    "        base_distribution = distrax.Independent(\n",
    "            distrax.Normal(jnp.zeros(n_dimensions), jnp.ones(n_dimensions)),\n",
    "            reinterpreted_batch_ndims=1,\n",
    "        )\n",
    "        pushforward = TransformedDistribution(\n",
    "            base_distribution, transform\n",
    "        )\n",
    "\n",
    "        return pushforward(**kwargs)\n",
    "\n",
    "    td = hk.transform(flow)\n",
    "    return td"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccf12b3-78c1-4f54-9a5a-fe74a8c12314",
   "metadata": {},
   "source": [
    "Training of the baseline is done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ae65ca9-3609-4680-931e-8d2021781b18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:33<00:00, 10.64it/s]\n"
     ]
    }
   ],
   "source": [
    "baseline = make_baseline(p_data)\n",
    "data = namedtuple(\"named_dataset\", \"y\")(y[:n_train])\n",
    "params_baseline, _ = train(next(rng_key_seq), data, baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184f34aa-58d3-47c1-a6f5-07dff5ec8863",
   "metadata": {},
   "source": [
    "## A surjective MLP funnel\n",
    "\n",
    "As a first surjective flow, we implement a `MLPInferenceFunnel`. The surjection uses a LU decomposition as inner bijector and a conditional probability density parameterized by an MLP as a decoder. We again use a flow of five layers. The first two and the last two are dimensionality-preserving `LULinear` bijections. The layer in the middle is a dimensionality-reducing funnel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69b8b1f2-c52b-40ed-8fd8-ad4b3e878554",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_surjective_mlp_funnel(n_dimensions):\n",
    "    def flow(**kwargs):\n",
    "        def decoder_fn(n_dim):\n",
    "            def fn(z):\n",
    "                params = make_mlp([32, 32, n_dim * 2])(z)\n",
    "                mu, log_scale = jnp.split(params, 2, -1)\n",
    "                return distrax.Independent(distrax.Normal(mu, jnp.exp(log_scale)))\n",
    "\n",
    "            return fn\n",
    "\n",
    "        n_dim = n_dimensions\n",
    "        layers = []\n",
    "        for i in range(5):\n",
    "            if i == 2:\n",
    "                layer = MLPInferenceFunnel(\n",
    "                    n_keep=int(n_dim / 2),\n",
    "                    decoder=decoder_fn(int(n_dim / 2))\n",
    "                )\n",
    "                n_dim = int(n_dim / 2)\n",
    "            else:\n",
    "                layer = LULinear(n_dim)\n",
    "            layers.append(layer)\n",
    "\n",
    "        transform = Chain(layers)\n",
    "        base_distribution = distrax.Independent(\n",
    "            distrax.Normal(jnp.zeros(n_dim), jnp.ones(n_dim)),\n",
    "            reinterpreted_batch_ndims=1,\n",
    "        )\n",
    "        pushforward = TransformedDistribution(\n",
    "            base_distribution, transform\n",
    "        )\n",
    "        return pushforward(**kwargs)\n",
    "\n",
    "    td = hk.transform(flow)\n",
    "    return td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac00eef9-595c-4689-af0c-74f8c1ad4f04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 165.38it/s]\n"
     ]
    }
   ],
   "source": [
    "surjective_mlp_funnel = make_surjective_mlp_funnel(p_data)\n",
    "data = namedtuple(\"named_dataset\", \"y\")(y[:n_train])\n",
    "params_surjective_mlp_funnel, _ = train(next(rng_key_seq), data, surjective_mlp_funnel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8a0587-81ba-4527-ae6a-e304051134a2",
   "metadata": {},
   "source": [
    "## A surjective affine masked coupling flow\n",
    "\n",
    "As a second surjector, we implement a `MaskedCouplingInferenceFunnel` with affine transformations. The surjection uses an affine masked coupling layer as inner bijector and a conditional probability density parameterized by an MLP as a decoder. We use the surjection in the middle of five flow layers. The other four are conventional masked coupling flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "689db19f-890d-4607-9caf-0e6782862407",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_surjective_affine_masked_coupling(n_dimensions):\n",
    "    def flow(**kwargs):\n",
    "        def bijector_fn(params):\n",
    "            means, log_scales = jnp.split(params, 2, -1)\n",
    "            return distrax.ScalarAffine(means, jnp.exp(log_scales))\n",
    "\n",
    "        def decoder_fn(n_dim):\n",
    "            def fn(z):\n",
    "                params = make_mlp([32, 32, n_dim * 2])(z)\n",
    "                mu, log_scale = jnp.split(params, 2, -1)\n",
    "                return distrax.Independent(distrax.Normal(mu, jnp.exp(log_scale)))\n",
    "\n",
    "            return fn\n",
    "\n",
    "        n_dim = n_dimensions\n",
    "        layers = []\n",
    "        for i in range(5):\n",
    "            if i == 2:\n",
    "                layer = MaskedCouplingInferenceFunnel(\n",
    "                    n_keep=int(n_dim / 2),\n",
    "                    decoder=decoder_fn(int(n_dim / 2)),\n",
    "                    conditioner=make_mlp([128, 128, 2 * n_dim]),\n",
    "                    bijector_fn=bijector_fn,\n",
    "                )\n",
    "                n_dim = int(n_dim / 2)\n",
    "            else:\n",
    "                layer = MaskedCoupling(\n",
    "                    mask=make_alternating_binary_mask(n_dim, i % 2 == 0),\n",
    "                    conditioner=make_mlp([128, 128, 2 * n_dim]),\n",
    "                    bijector_fn=bijector_fn,\n",
    "                )\n",
    "            layers.append(layer)\n",
    "\n",
    "        transform = Chain(layers)\n",
    "        base_distribution = distrax.Independent(\n",
    "            distrax.Normal(jnp.zeros(n_dim), jnp.ones(n_dim)),\n",
    "            reinterpreted_batch_ndims=1,\n",
    "        )\n",
    "        pushforward = TransformedDistribution(\n",
    "            base_distribution, transform\n",
    "        )\n",
    "        return pushforward(**kwargs)\n",
    "\n",
    "    td = hk.transform(flow)\n",
    "    return td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33f1a4c9-8768-417a-b41f-3d459ec473e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:26<00:00, 37.56it/s]\n"
     ]
    }
   ],
   "source": [
    "surjective_affine_masked_coupling = make_surjective_affine_masked_coupling(p_data)\n",
    "data = namedtuple(\"named_dataset\", \"y\")(y[:n_train])\n",
    "params_surjective_affine_masked_coupling, _ = train(next(rng_key_seq), data, surjective_affine_masked_coupling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd46c66-4437-4c48-9e4a-7e743cd33926",
   "metadata": {},
   "source": [
    "## A surjective rational quadratic masked coupling flow\n",
    "\n",
    "Finally, we implement a `MaskedCouplingInferenceFunnel` with a rational quadratic transformations. The flow is the same as before, but with affine transformations replaced with splines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "516294ba-ac7a-40fc-a6c8-04a4d5964ff3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_surjective_rq_masked_coupling(n_dimensions):\n",
    "    def flow(**kwargs):\n",
    "        def bijector_fn(params):\n",
    "            return distrax.RationalQuadraticSpline(params, range_min=range_min, range_max=range_max)\n",
    "\n",
    "        def decoder_fn(n_dim):\n",
    "            def fn(z):\n",
    "                params = make_mlp([32, 32, n_dim * 2])(z)\n",
    "                mu, log_scale = jnp.split(params, 2, -1)\n",
    "                return distrax.Independent(distrax.Normal(mu, jnp.exp(log_scale)))\n",
    "\n",
    "            return fn\n",
    "\n",
    "        n_dim = n_dimensions\n",
    "        layers = []\n",
    "        for i in range(5):\n",
    "            if i == 2:\n",
    "                layer = MaskedCouplingInferenceFunnel(\n",
    "                    n_keep=int(n_dim / 2),\n",
    "                    decoder=decoder_fn(int(n_dim / 2)),\n",
    "                    conditioner=make_rq_conditioner(n_dim, [128, 128], 4),\n",
    "                    bijector_fn=bijector_fn,\n",
    "                )\n",
    "                n_dim = int(n_dim / 2)\n",
    "            else:\n",
    "                layer = MaskedCoupling(\n",
    "                    mask=make_alternating_binary_mask(n_dim, i % 2 == 0),\n",
    "                    conditioner=make_rq_conditioner(n_dim, [128, 128], 4),\n",
    "                    bijector_fn=bijector_fn,\n",
    "                )\n",
    "            layers.append(layer)\n",
    "\n",
    "        transform = Chain(layers)\n",
    "        base_distribution = distrax.Independent(\n",
    "            distrax.Normal(jnp.zeros(n_dim), jnp.ones(n_dim)),\n",
    "            reinterpreted_batch_ndims=1,\n",
    "        )\n",
    "        pushforward = TransformedDistribution(\n",
    "            base_distribution, transform\n",
    "        )\n",
    "        return pushforward(**kwargs)\n",
    "\n",
    "    td = hk.transform(flow)\n",
    "    return td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72624343-491a-46be-8aae-ed4a4daf3657",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:22<00:00, 12.11it/s]\n"
     ]
    }
   ],
   "source": [
    "surjective_rq_masked_coupling = make_surjective_rq_masked_coupling(p_data)\n",
    "data = namedtuple(\"named_dataset\", \"y\")(y[:n_train])\n",
    "params_surjective_rq_masked_coupling, _ = train(next(rng_key_seq), data, surjective_rq_masked_coupling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2ee4bc-3e93-4164-84d0-cfd55e1536a3",
   "metadata": {},
   "source": [
    "## Density comparisons\n",
    "\n",
    "Having trained the baseline and surjectors, let’s compute density estimates of the training and test data sets using the four models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03b973b4-f8d6-4c7f-b7b1-4f8c016263a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_list = [\n",
    "    baseline,\n",
    "    surjective_mlp_funnel,\n",
    "    surjective_affine_masked_coupling,\n",
    "    surjective_rq_masked_coupling\n",
    "]\n",
    "\n",
    "param_list = [\n",
    "    params_baseline,\n",
    "    params_surjective_mlp_funnel,\n",
    "    params_surjective_affine_masked_coupling,\n",
    "    params_surjective_rq_masked_coupling,\n",
    "]\n",
    "\n",
    "lps = []\n",
    "for model, params in zip(model_list, param_list):\n",
    "    lp_training = model.apply(params, None, method=\"log_prob\", y=y[:n_train])\n",
    "    lp_test = model.apply(params, None, method=\"log_prob\", y=y[n_train:])\n",
    "    lp_training = jnp.mean(lp_training)\n",
    "    lp_test = jnp.mean(lp_test)\n",
    "    lps.append(np.array([lp_training, lp_test]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc30ef2-87c9-4f4a-97ff-f0b8225b92c2",
   "metadata": {},
   "source": [
    "Not so surprisingly the MLP funnel works best on this data set. The baseline that does not reduce dimensionality has the worst performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22f1e018-b643-402f-ad3c-929ab888abbf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Training density</th>\n",
       "      <th>Test density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>2.973894</td>\n",
       "      <td>1.514763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLP funnel</td>\n",
       "      <td>10.492462</td>\n",
       "      <td>10.487741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Affine masked coupling funnel</td>\n",
       "      <td>10.481473</td>\n",
       "      <td>10.244188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RQ masked coupling funnel</td>\n",
       "      <td>6.814237</td>\n",
       "      <td>5.800568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Model  Training density  Test density\n",
       "0                       Baseline          2.973894      1.514763\n",
       "1                     MLP funnel         10.492462     10.487741\n",
       "2  Affine masked coupling funnel         10.481473     10.244188\n",
       "3      RQ masked coupling funnel          6.814237      5.800568"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(lps, columns=[\"Training density\", \"Test density\"])\n",
    "df.insert(0, \"Model\", [\"Baseline\", \"MLP funnel\", \"Affine masked coupling funnel\", \"RQ masked coupling funnel\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee5f926-6b37-42e2-9344-5e08352ac826",
   "metadata": {},
   "source": [
    "## Session info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "431bc3fc-8e97-4fa4-b454-ace055e8fe1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "distrax             0.1.4\n",
      "haiku               0.0.10\n",
      "jax                 0.4.18\n",
      "jaxlib              0.4.18\n",
      "matplotlib          3.6.3\n",
      "numpy               1.25.2\n",
      "optax               0.1.7\n",
      "pandas              1.5.1\n",
      "seaborn             0.12.2\n",
      "session_info        1.0.0\n",
      "sklearn             1.2.1\n",
      "surjectors          0.3.0\n",
      "tqdm                4.64.1\n",
      "-----\n",
      "IPython             8.12.0\n",
      "jupyter_client      8.2.0\n",
      "jupyter_core        5.3.0\n",
      "jupyterlab          3.6.3\n",
      "notebook            6.5.4\n",
      "-----\n",
      "Python 3.9.13 | packaged by conda-forge | (main, May 27 2022, 17:01:00) [Clang 13.0.1 ]\n",
      "macOS-13.0.1-arm64-arm-64bit\n",
      "-----\n",
      "Session information updated at 2023-10-14 23:52\n"
     ]
    }
   ],
   "source": [
    "import session_info\n",
    "session_info.show(html=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35c40b8-29e0-4518-bd42-94d0ad1dd1c3",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Klein, Samuel, et al. “Funnels: Exact maximum likelihood with dimensionality reduction”. Workshop on Bayesian Deep Learning, Advances in Neural Information Processing Systems, 2021.\n",
    "\n",
    "[2] Nielsen, Didrik, et al. “SurVAE Flows: Surjections to Bridge the Gap between VAEs and Flows”. Advances in Neural Information Processing Systems, 2020."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esurjectors-dev",
   "language": "python",
   "name": "esurjectors-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
