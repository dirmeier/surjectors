{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f6f4229-6b15-4e2b-89af-8957708479d7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Dimensionality reduction with surjections\n",
    "\n",
    "Surjective normalizing flows use dimensionality-reducing transformations instead of dimensionality-preserving bijective ones. \n",
    "Below we implement several surjective normalizing for a density estimation problem and compare them to a conventional bijective flow.\n",
    "\n",
    "Interactive online version:\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/dirmeier/surjectors/blob/main/docs/notebooks/dimension_reduction.ipynb\">\n",
    "    <img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"vertical-align:text-bottom\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import distrax\n",
    "import haiku as hk\n",
    "import jax\n",
    "import numpy as np\n",
    "import optax\n",
    "import pandas as pd\n",
    "\n",
    "from collections import namedtuple\n",
    "from jax import jit\n",
    "from jax import numpy as jnp\n",
    "from jax import random as jr\n",
    "from tqdm import tqdm\n",
    "\n",
    "from surjectors import (\n",
    "    Chain,\n",
    "    LULinear,\n",
    "    MaskedCoupling,\n",
    "    MaskedCouplingInferenceFunnel,\n",
    "    MLPInferenceFunnel,\n",
    "    TransformedDistribution,\n",
    ")\n",
    "from surjectors.nn import make_mlp\n",
    "from surjectors.util import (\n",
    "    as_batch_iterator,\n",
    "    make_alternating_binary_mask,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We define a training function first that we can use for all density estimation tasks below."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(rng_key, data, model, n_iter=1000):\n",
    "    # convert the data set to an iterator\n",
    "    batch_key, rng_key = jr.split(rng_key)\n",
    "    train_iter = as_batch_iterator(batch_key, data, 100, True)\n",
    "\n",
    "    # initialize the model\n",
    "    init_key, rng_key = jr.split(rng_key)\n",
    "    params = model.init(init_key, method=\"log_prob\", **train_iter(0))\n",
    "\n",
    "    # create an optimizer\n",
    "    optimizer = optax.adam(1e-4)\n",
    "    state = optimizer.init(params)\n",
    "\n",
    "    @jit\n",
    "    # gradient step\n",
    "    def step(params, state, **batch):\n",
    "        def loss_fn(params):\n",
    "            lp = model.apply(params, None, method=\"log_prob\", **batch)\n",
    "            return -jnp.mean(lp)\n",
    "\n",
    "        loss, grads = jax.value_and_grad(loss_fn)(params)\n",
    "        updates, new_state = optimizer.update(grads, state, params)\n",
    "        new_params = optax.apply_updates(params, updates)\n",
    "        return loss, new_params, new_state\n",
    "\n",
    "    losses = np.zeros(n_iter)\n",
    "    # training loop\n",
    "    for i in tqdm(range(n_iter)):\n",
    "        train_loss = 0.0\n",
    "        # iterate over batches\n",
    "        for j in range(train_iter.num_batches):\n",
    "            batch = train_iter(j)\n",
    "            batch_loss, params, state = step(params, state, **batch)\n",
    "            train_loss += batch_loss\n",
    "        losses[i] = train_loss\n",
    "\n",
    "    return params, losses"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Data\n",
    "\n",
    "We simulate data from a factor model for testing. The data can should easily embedded in a lower dimensional space via a linear transformation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rng_key_seq = hk.PRNGSequence(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_train, n_test = 1000, 200\n",
    "n = n_train + n_test\n",
    "p_data, p_latent = 20, 5\n",
    "\n",
    "z = jr.normal(next(rng_key_seq), (n, p_latent))\n",
    "W = jr.normal(next(rng_key_seq), (p_data, p_latent)) * 0.1\n",
    "y = (W @ z.T).T + jr.normal(next(rng_key_seq), (n, p_data)) * 0.1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A bijective baseline\n",
    "\n",
    "We start with a simple baseline: a masked coupling flow with rational quadratic splines as transforms. The data is not dimensionality-reducing and hence tries to estimate the density on the $20$-dimensional space."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A RQ splince flow requires defining ranges for which we use the lower and upper bounds of the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "range_min, range_max = float(np.min(y)), float(np.max(y))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we define the conditioner function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_rq_conditioner(event_shape, hidden_sizes, n_bins):\n",
    "    n_params = 3 * n_bins + 1\n",
    "    return hk.Sequential(\n",
    "        [\n",
    "            make_mlp(hidden_sizes + [event_shape * n_params]),\n",
    "            hk.Reshape((event_shape,) + (n_params,), preserve_dims=-1),\n",
    "        ]\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We create a baseine that uses five masked coupling flows."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_baseline(n_dimensions):\n",
    "    def flow(**kwargs):\n",
    "        def bijector_fn(params):\n",
    "            return distrax.RationalQuadraticSpline(\n",
    "                params, range_min=range_min, range_max=range_max\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        for i in range(5):\n",
    "            layer = MaskedCoupling(\n",
    "                mask=make_alternating_binary_mask(n_dimensions, i % 2 == 0),\n",
    "                conditioner=make_rq_conditioner(n_dimensions, [128, 128], 4),\n",
    "                bijector_fn=bijector_fn,\n",
    "            )\n",
    "            layers.append(layer)\n",
    "\n",
    "        transform = Chain(layers)\n",
    "        base_distribution = distrax.Independent(\n",
    "            distrax.Normal(jnp.zeros(n_dimensions), jnp.ones(n_dimensions)),\n",
    "            reinterpreted_batch_ndims=1,\n",
    "        )\n",
    "        pushforward = TransformedDistribution(base_distribution, transform)\n",
    "\n",
    "        return pushforward(**kwargs)\n",
    "\n",
    "    td = hk.transform(flow)\n",
    "    return td"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training of the baseline is done as follows:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "baseline = make_baseline(p_data)\n",
    "data = namedtuple(\"named_dataset\", \"y\")(y[:n_train])\n",
    "params_baseline, _ = train(next(rng_key_seq), data, baseline)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A surjective MLP funnel\n",
    "\n",
    "As a first surjective flow, we implement a `MLPInferenceFunnel`. The surjection uses a LU decomposition as inner bijector and a conditional probability density parameterized by an MLP as a decoder. We again use a flow of five layers. The first two and the last two are dimensionality-preserving `LULinear` bijections. The layer in the middle is a dimensionality-reducing funnel."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_surjective_mlp_funnel(n_dimensions):\n",
    "    def flow(**kwargs):\n",
    "        def decoder_fn(n_dim):\n",
    "            def fn(z):\n",
    "                params = make_mlp([32, 32, n_dim * 2])(z)\n",
    "                mu, log_scale = jnp.split(params, 2, -1)\n",
    "                return distrax.Independent(\n",
    "                    distrax.Normal(mu, jnp.exp(log_scale))\n",
    "                )\n",
    "\n",
    "            return fn\n",
    "\n",
    "        n_dim = n_dimensions\n",
    "        layers = []\n",
    "        for i in range(5):\n",
    "            if i == 2:\n",
    "                layer = MLPInferenceFunnel(\n",
    "                    n_keep=int(n_dim / 2), decoder=decoder_fn(int(n_dim / 2))\n",
    "                )\n",
    "                n_dim = int(n_dim / 2)\n",
    "            else:\n",
    "                layer = LULinear(n_dim)\n",
    "            layers.append(layer)\n",
    "\n",
    "        transform = Chain(layers)\n",
    "        base_distribution = distrax.Independent(\n",
    "            distrax.Normal(jnp.zeros(n_dim), jnp.ones(n_dim)),\n",
    "            reinterpreted_batch_ndims=1,\n",
    "        )\n",
    "        pushforward = TransformedDistribution(base_distribution, transform)\n",
    "        return pushforward(**kwargs)\n",
    "\n",
    "    td = hk.transform(flow)\n",
    "    return td"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "surjective_mlp_funnel = make_surjective_mlp_funnel(p_data)\n",
    "data = namedtuple(\"named_dataset\", \"y\")(y[:n_train])\n",
    "params_surjective_mlp_funnel, _ = train(\n",
    "    next(rng_key_seq), data, surjective_mlp_funnel\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A surjective affine masked coupling flow\n",
    "\n",
    "As a second surjector, we implement a `MaskedCouplingInferenceFunnel` with affine transformations. The surjection uses an affine masked coupling layer as inner bijector and a conditional probability density parameterized by an MLP as a decoder. We use the surjection in the middle of five flow layers. The other four are conventional masked coupling flows."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_surjective_affine_masked_coupling(n_dimensions):\n",
    "    def flow(**kwargs):\n",
    "        def bijector_fn(params):\n",
    "            means, log_scales = jnp.split(params, 2, -1)\n",
    "            return distrax.ScalarAffine(means, jnp.exp(log_scales))\n",
    "\n",
    "        def decoder_fn(n_dim):\n",
    "            def fn(z):\n",
    "                params = make_mlp([32, 32, n_dim * 2])(z)\n",
    "                mu, log_scale = jnp.split(params, 2, -1)\n",
    "                return distrax.Independent(\n",
    "                    distrax.Normal(mu, jnp.exp(log_scale))\n",
    "                )\n",
    "\n",
    "            return fn\n",
    "\n",
    "        n_dim = n_dimensions\n",
    "        layers = []\n",
    "        for i in range(5):\n",
    "            if i == 2:\n",
    "                layer = MaskedCouplingInferenceFunnel(\n",
    "                    n_keep=int(n_dim / 2),\n",
    "                    decoder=decoder_fn(int(n_dim / 2)),\n",
    "                    conditioner=make_mlp([128, 128, 2 * n_dim]),\n",
    "                    bijector_fn=bijector_fn,\n",
    "                )\n",
    "                n_dim = int(n_dim / 2)\n",
    "            else:\n",
    "                layer = MaskedCoupling(\n",
    "                    mask=make_alternating_binary_mask(n_dim, i % 2 == 0),\n",
    "                    conditioner=make_mlp([128, 128, 2 * n_dim]),\n",
    "                    bijector_fn=bijector_fn,\n",
    "                )\n",
    "            layers.append(layer)\n",
    "\n",
    "        transform = Chain(layers)\n",
    "        base_distribution = distrax.Independent(\n",
    "            distrax.Normal(jnp.zeros(n_dim), jnp.ones(n_dim)),\n",
    "            reinterpreted_batch_ndims=1,\n",
    "        )\n",
    "        pushforward = TransformedDistribution(base_distribution, transform)\n",
    "        return pushforward(**kwargs)\n",
    "\n",
    "    td = hk.transform(flow)\n",
    "    return td"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "surjective_affine_masked_coupling = make_surjective_affine_masked_coupling(\n",
    "    p_data\n",
    ")\n",
    "data = namedtuple(\"named_dataset\", \"y\")(y[:n_train])\n",
    "params_surjective_affine_masked_coupling, _ = train(\n",
    "    next(rng_key_seq), data, surjective_affine_masked_coupling\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A surjective rational quadratic masked coupling flow\n",
    "\n",
    "Finally, we implement a `MaskedCouplingInferenceFunnel` with a rational quadratic transformations. The flow is the same as before, but with affine transformations replaced with splines."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_surjective_rq_masked_coupling(n_dimensions):\n",
    "    def flow(**kwargs):\n",
    "        def bijector_fn(params):\n",
    "            return distrax.RationalQuadraticSpline(\n",
    "                params, range_min=range_min, range_max=range_max\n",
    "            )\n",
    "\n",
    "        def decoder_fn(n_dim):\n",
    "            def fn(z):\n",
    "                params = make_mlp([32, 32, n_dim * 2])(z)\n",
    "                mu, log_scale = jnp.split(params, 2, -1)\n",
    "                return distrax.Independent(\n",
    "                    distrax.Normal(mu, jnp.exp(log_scale))\n",
    "                )\n",
    "\n",
    "            return fn\n",
    "\n",
    "        n_dim = n_dimensions\n",
    "        layers = []\n",
    "        for i in range(5):\n",
    "            if i == 2:\n",
    "                layer = MaskedCouplingInferenceFunnel(\n",
    "                    n_keep=int(n_dim / 2),\n",
    "                    decoder=decoder_fn(int(n_dim / 2)),\n",
    "                    conditioner=make_rq_conditioner(n_dim, [128, 128], 4),\n",
    "                    bijector_fn=bijector_fn,\n",
    "                )\n",
    "                n_dim = int(n_dim / 2)\n",
    "            else:\n",
    "                layer = MaskedCoupling(\n",
    "                    mask=make_alternating_binary_mask(n_dim, i % 2 == 0),\n",
    "                    conditioner=make_rq_conditioner(n_dim, [128, 128], 4),\n",
    "                    bijector_fn=bijector_fn,\n",
    "                )\n",
    "            layers.append(layer)\n",
    "\n",
    "        transform = Chain(layers)\n",
    "        base_distribution = distrax.Independent(\n",
    "            distrax.Normal(jnp.zeros(n_dim), jnp.ones(n_dim)),\n",
    "            reinterpreted_batch_ndims=1,\n",
    "        )\n",
    "        pushforward = TransformedDistribution(base_distribution, transform)\n",
    "        return pushforward(**kwargs)\n",
    "\n",
    "    td = hk.transform(flow)\n",
    "    return td"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "surjective_rq_masked_coupling = make_surjective_rq_masked_coupling(p_data)\n",
    "data = namedtuple(\"named_dataset\", \"y\")(y[:n_train])\n",
    "params_surjective_rq_masked_coupling, _ = train(\n",
    "    next(rng_key_seq), data, surjective_rq_masked_coupling\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Density comparisons\n",
    "\n",
    "Having trained the baseline and surjectors, let’s compute density estimates of the training and test data sets using the four models."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_list = [\n",
    "    baseline,\n",
    "    surjective_mlp_funnel,\n",
    "    surjective_affine_masked_coupling,\n",
    "    surjective_rq_masked_coupling,\n",
    "]\n",
    "\n",
    "param_list = [\n",
    "    params_baseline,\n",
    "    params_surjective_mlp_funnel,\n",
    "    params_surjective_affine_masked_coupling,\n",
    "    params_surjective_rq_masked_coupling,\n",
    "]\n",
    "\n",
    "lps = []\n",
    "for model, params in zip(model_list, param_list):\n",
    "    lp_training = model.apply(params, None, method=\"log_prob\", y=y[:n_train])\n",
    "    lp_test = model.apply(params, None, method=\"log_prob\", y=y[n_train:])\n",
    "    lp_training = jnp.mean(lp_training)\n",
    "    lp_test = jnp.mean(lp_test)\n",
    "    lps.append(np.array([lp_training, lp_test]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Not so surprisingly the MLP funnel works best on this data set. The baseline that does not reduce dimensionality has the worst performance."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(lps, columns=[\"Training density\", \"Test density\"])\n",
    "df.insert(\n",
    "    0,\n",
    "    \"Model\",\n",
    "    [\n",
    "        \"Baseline\",\n",
    "        \"MLP funnel\",\n",
    "        \"Affine masked coupling funnel\",\n",
    "        \"RQ masked coupling funnel\",\n",
    "    ],\n",
    ")\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Session info"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esurjectors-dev",
   "language": "python",
   "name": "esurjectors-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}